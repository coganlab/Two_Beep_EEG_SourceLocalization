{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Jan 3 2021\n",
    "\n",
    "@author: aidanrogers\n",
    "\"\"\"\n",
    "\n",
    "import mne\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib qt\n",
    "import os, sys\n",
    "#import keyboard\n",
    "import time\n",
    "import pathlib\n",
    "from autoreject import AutoReject\n",
    "import os\n",
    "from os.path import join\n",
    "from os import chdir\n",
    "\n",
    "#mne.viz.set_3d_options(antialias=False, depth_peeling = False, smooth_shading = False)\n",
    "#mne.viz.set_3d_backend('pyvista')\n",
    "\n",
    "if os.name == 'posix': # linux\n",
    "    D_Drive = '/mnt/d/Aidan'\n",
    "    root_dir = '~'\n",
    "    SUBJECTS_DIR = os.getenv('SUBJECTS_DIR')\n",
    "    FREESURFER_HOME = os.getenv('FREESURFER_HOME')\n",
    "elif os.name == 'nt': # windows\n",
    "    D_Drive = 'D:\\Aidan'\n",
    "    root_dir = join(\"C:\\\\\",\"Users\",\"aar40\",\"AppData\",\"Local\",\"Packages\",\n",
    "                           \"CanonicalGroupLimited.Ubuntu22.04LTS_79rhkp1fndgsc\",\"LocalState\",\"rootfs\")\n",
    "    FREESURFER_HOME = join(root_dir, \"usr\", \"local\", \"freesurfer\", \"7.3.2\")\n",
    "    SUBJECTS_DIR = join(FREESURFER_HOME,'subjects')\n",
    "    os.environ['SUBJECTS_DIR'] = SUBJECTS_DIR\n",
    "    os.environ['FREESURFER_HOME'] = FREESURFER_HOME\n",
    "else:\n",
    "    raise SystemError('Unknown system os')\n",
    "\n",
    "mne.viz.set_3d_options(antialias=False)\n",
    "mne.viz.set_3d_backend('pyvista')\n",
    "\n",
    "#cd D:\\Aidan\n",
    "%matplotlib qt\n",
    "\n",
    "#D:/Aidan/E1/BrainVision_Recorder/E1_TwoBeeps.vhdr\n",
    "#124_Ch_Test.bvef\n",
    "\n",
    "def sleeper():\n",
    "    \"\"\"\n",
    "    Attempted function to streamline code\n",
    "    \n",
    "    function was used to try and cause pauses between opening plots \n",
    "    for EEG analysis\n",
    "\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        num = 300\n",
    "        \n",
    "        try:\n",
    "            num = float(num)\n",
    "        except ValueError:\n",
    "            print('Please enter in a number.\\n')\n",
    "            continue\n",
    "        print('before: %s' % time.ctime())\n",
    "        time.sleep(num)\n",
    "        print('After: %s\\n' % time.ctime())\n",
    "    \n",
    "\n",
    "def make_montage():\n",
    "    \"\"\"\n",
    "    creates montage, identifies and finds the montage, then makes sure data \n",
    "    is properly aligned with electrode placement we want \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    montage_fname : montage\n",
    "        DESCRIPTION - electrode placement on head of specific subj (128)\n",
    "    subj_num : TYPE\n",
    "        DESCRIPTION - the subject # - who's file are we accessing?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    montage : montage\n",
    "        DESCRIPTION  - aligned montage\n",
    "    raw : raw\n",
    "        DESCRIPTION - bandpass filtered raw data file\n",
    "\n",
    "    \"\"\"\n",
    "    # have to be in same directory as the file ur looking at to access b/c MNE\n",
    "    os.chdir(D_Drive)\n",
    "    \n",
    "    \n",
    "    montage_fname = input ('Are you using a custom (C) or default (D) montage/electrode placement ')\n",
    "    subj_num = input('what subject # are you analyzing: ')\n",
    "    \n",
    "    \n",
    "    # to be used for the inverse solution (making the bem, coregistraton, covariance etc)\n",
    "    global subj \n",
    "    subj = 'E'+ subj_num\n",
    "    global subject\n",
    "    subject = subj\n",
    "    \n",
    "    # to be used for the inverse solution (making the bem, coregistraton, covariance etc)\n",
    "    # global subjects_dir\n",
    "    # global FREESURFER_HOME\n",
    "    # subjects_dir = 'C:/Users/aar40/Desktop/share/freesurfer/subjects'\n",
    "    # FREESURFER_HOME = 'C:/Users/aar40/Desktop/share/freesurfer'\n",
    "\n",
    "    # mne.set_config(key = subjects_dir, value = 'E46')\n",
    "    \n",
    "    # to identify the correct montage/electrode config\n",
    "    if (montage_fname == 'D'):\n",
    "        montage_name = '124_Ch_Test.bvef'\n",
    "        #load default montage / electrode position\n",
    "        montage = mne.channels.read_custom_montage(montage_name)\n",
    "    elif (montage_fname == 'C'):\n",
    "        #load custom montage / electrode position\n",
    "        montage_name = traverse('CapTrak', subj, subj, '.bvct')\n",
    "        montage = mne.channels.read_dig_captrak(montage_name)\n",
    "    else:\n",
    "        print('invalid input')\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    vhdr1 = 'TwoBeeps' #change these to change what type of file you're analyzinbg\n",
    "    vhdr2 = '.vhdr'\n",
    "    vhdr_fname = traverse(os.sep + 'BrainVision_Recorder', subj, vhdr1, vhdr2)\n",
    "   # vhdr_fname = ''\n",
    "\n",
    "    #vhdr_path = 'D:\\Aidan/' + subj + '/BrainVision_Recorder/'     \n",
    "    #vhdr_fname = (filenames for _,_,filenames in os.walk(vhdr_path) if vhdr1 & vhdr2 in filenames)\n",
    "    raw = mne.io.read_raw_brainvision(vhdr_fname, preload=True)\n",
    "    \n",
    "    #Channel renaming dictionary for TwoBeeps subjs E2 - 27\n",
    "    if ((int(subj_num) > 1) & (int(subj_num) < 28 )):\n",
    "        raw = load_data_skewed(raw);\n",
    "    \n",
    "    if (subj != 'E1'): #set eog\n",
    "        raw.set_channel_types({'125':'eog','126':'eog','127':'eog','128':'eog'})\n",
    "        \n",
    "    #apply bandpass filter, set frequency bands\n",
    "    raw.filter(0.5,40)\n",
    "    \n",
    "    #after filtering, remove bad channels in qt plot\n",
    "    raw.plot()\n",
    "    # print(\"wait\")\n",
    "    # wait = input('Press a key to continue: ')\n",
    "    # print(\"continue\")\n",
    "    #cont = keyboard.read_key()\n",
    "    #raw.set_eeg_reference('average')\n",
    "    # raw.set_eeg_reference('average', projection=True)\n",
    "\n",
    "    # raw.set_montage(montage)\n",
    "    # raw.save(pathlib.Path('/mnt/d/Aidan/' + subj + '/Digitization') / 'Captrak_Digitization.fif', overwrite = True)\n",
    "    \n",
    "    return montage, raw\n",
    "\n",
    "def traverse(where, subj, vhdr1, vhdr2):\n",
    "    global vhdr_fname_1\n",
    "    # for dirpath, dirnames, filenames in os.walk('D:\\Aidan/' + subj + where):\n",
    "    for dirpath, dirnames, filenames in os.walk(D_Drive + os.sep + subj + os.sep + where):\n",
    "        for name in filenames:\n",
    "            if vhdr1 in name:\n",
    "                if vhdr2 in name:\n",
    "                    vhdr_fname_1 = dirpath + os.sep + name \n",
    "                    print(vhdr_fname_1)\n",
    "                    print(type(vhdr_fname_1))\n",
    "                    return vhdr_fname_1\n",
    "\n",
    "def load_data_skewed(raw):\n",
    "    '''\n",
    "    takes in raw data file and makes sure the channels (which were missnamed)\n",
    "    are re-aligned given a dictionary\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    raw : raw\n",
    "        DESCRIPTION - raw data file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    raw : raw\n",
    "        DESCRIPTION - data aligned with correct electrode orientation\n",
    "\n",
    "    '''\n",
    "    #Load in EEG file\n",
    "    # raw = mne.io.read_raw_brainvision(vhdr_fname, preload=True)\n",
    "\n",
    "    #Channel renaming dictionary for TwoBeeps subjs E2 - 27\n",
    "    #1-32:97:128, 33-64:65-96, 65-96:33-64. 97-128:1-32\n",
    "    name_dict = {'1':'97','2':'98','3':'99','4':'100','5':'101','6':'102',\n",
    "             '7':'103','8':'104','9':'105','10':'106','11':'107','12':'108',\n",
    "             '13':'109','14':'110','15':'111','16':'112','17':'113',\n",
    "             '18':'114','19':'115','20':'116','21':'117','22':'118',\n",
    "             '23':'119','24':'120','25':'121','26':'122','27':'123',\n",
    "             '28':'124','29':'125','30':'126','31':'127','32':'128','33':'65',\n",
    "             '34':'66','35':'67','36':'68','37':'69','38':'70','39':'71',\n",
    "             '40':'72','41':'73','42':'74','43':'75','44':'76','45':'77',\n",
    "             '46':'78','47':'79','48':'80','49':'81','50':'82','51':'83',\n",
    "             '52':'84','53':'85','54':'86','55':'87','56':'88','57':'89',\n",
    "             '58':'90','59':'91','60':'92','61':'93','62':'94','63':'95',\n",
    "             '64':'96','65':'33','66':'34','67':'35','68':'36','69':'37',\n",
    "             '70':'38','71':'39','72':'40','73':'41','74':'42','75':'43',\n",
    "             '76':'44','77':'45','78':'46','79':'47','80':'48','81':'49',\n",
    "             '82':'50','83':'51','84':'52','85':'53','86':'54','87':'55',\n",
    "             '88':'56','89':'57','90':'58','91':'59','92':'60','93':'61',\n",
    "             '94':'62','95':'63','96':'64','97':'1','98':'2','99':'3',\n",
    "             '100':'4','101':'5','102':'6','103':'7','104':'8','105':'9',\n",
    "             '106':'10','107':'11','108':'12','109':'13','110':'14',\n",
    "             '111':'15','112':'16','113':'17','114':'18','115':'19',\n",
    "             '116':'20','117':'21','118':'22','119':'23','120':'24',\n",
    "             '121':'25','122':'26','123':'27','124':'28','125':'29',\n",
    "             '126':'30','127':'31','128':'32'}\n",
    "    \n",
    "    #list to reorder channels after renaming\n",
    "    order_list = ['1','2','3','4','5','6','7','8','9','10','11','12','13',\n",
    "                  '14','15','16','17','18','19','20','21','22','23','24','25',\n",
    "                  '26','27','28','29','30','31','32','33','34','35','36','37',\n",
    "                  '38','39','40','41','42','43','44','45','46','47','48','49',\n",
    "                  '50','51','52','53','54','55','56','57','58','59','60','61',\n",
    "                  '62','63','64','65','66','67','68','69','70','71','72','73',\n",
    "                  '74','75','76','77','78','79','80','81','82','83','84','85',\n",
    "                  '86','87','88','89','90','91','92','93','94','95','96','97',\n",
    "                  '98','99','100','101','102','103','104','105','106','107',\n",
    "                  '108','109','110','111','112','113','114','115','116','117',\n",
    "                  '118','119','120','121','122','123','124','125','126','127',\n",
    "                  '128']\n",
    "\n",
    "    #rename channels based on above dictionary\n",
    "    raw.rename_channels(name_dict)\n",
    "    \n",
    "    #reorder channels to go sequentially from 1 to 128\n",
    "    raw.reorder_channels(order_list)\n",
    "    \n",
    "    return raw\n",
    "    \n",
    "    \n",
    "def set_data(raw, montage):\n",
    "    '''\n",
    "    common average reference.\n",
    "    take the average signal across all electrodes and take it off the top\n",
    "    reduces noise by 30% - on average\n",
    "    '''\n",
    "    #set reference to average    \n",
    "    raw.set_eeg_reference('average', projection=True)\n",
    "    \n",
    "    #plot data again to remove any remaining bad channels\n",
    "    raw.plot()\n",
    "    \n",
    "    #cont = keyboard.read_key()\n",
    "\n",
    "\n",
    "    return raw\n",
    "    \n",
    "\n",
    "def EOG_check(raw2, ica):\n",
    "    '''\n",
    "    1. regenerates epochs, and evoked (avg epochs)\n",
    "    2. uses find_bads_eog locations and saves those voltages and time periods\n",
    "    3. plot functions (some are commented out)\n",
    "    4. ica.exclude.extend(eog_indices) --> \n",
    "    --> plots newly excluded indices as a copy of raw data\n",
    "    5. display new ERP (doesn't save the data to the main raw file)\n",
    "    '''\n",
    "    eog_epochs=mne.preprocessing.create_eog_epochs(raw, baseline=(-0.5, -0.1))\n",
    "    # eog_epochs.plot_image(combine='mean')\n",
    "    # eog_epochs.average().plot_joint()\n",
    "    eog_average = eog_epochs.average()\n",
    "\n",
    "    # ica.exclude = []\n",
    "    # find which ICs match the EOG pattern\n",
    "    # eog_epochs = mne.preprocessing.create_eog_epochs(raw, baseline=(-0.5, -0.2))\n",
    "    [eog_indices,eog_scores]=ica.find_bads_eog(eog_epochs)\n",
    "    \n",
    "    # barplot of ICA component \"EOG match\" scores\n",
    "    # ica.plot_scores(eog_scores, exclude=eog_indices)\n",
    "    \n",
    "    # plot diagnostics\n",
    "    #ica.plot_properties(raw2, picks=eog_indices)\n",
    "    \n",
    "    # plot ICs applied to raw data, with EOG matches highlighted\n",
    "    # ica.plot_sources(raw2, show_scrollbars=True)\n",
    "    \n",
    "    # plot ICs applied to the averaged EOG epochs, with EOG matches highlighted\n",
    "    ica.plot_sources(eog_average)\n",
    "    \n",
    "    #ica.plot_properties(eog_epochs, picks=eog_indices, psd_args={'fmax': 35.},\n",
    "    #                image_args={'sigma': 1.})\n",
    "    #print(ica.labels_)\n",
    "    \n",
    "    ica.plot_overlay(eog_average, exclude=eog_indices, show=True, title = 'average_indices removed')\n",
    "    ica.plot_overlay(inst = raw2, exclude = eog_indices, title = 'raw before and blink detection')\n",
    "\n",
    "    \n",
    "    ica.exclude.extend(eog_indices)\n",
    "    \n",
    "    reconst_raw = raw2.copy()\n",
    "    ica.apply(inst=reconst_raw)\n",
    "    \n",
    "    erp(reconst_raw, montage)    \n",
    "    \n",
    "    return reconst_raw\n",
    "    \n",
    "\n",
    "def EOG_check_ar(epochs_ar, ica):\n",
    "    '''\n",
    "    1. regenerates epochs, and evoked (avg epochs)\n",
    "    2. uses find_bads_eog locations and saves those voltages and time periods\n",
    "    3. plot functions (some are commented out)\n",
    "    4. ica.exclude.extend(eog_indices) --> \n",
    "    --> plots newly excluded indices as a copy of raw data\n",
    "    5. display new ERP (doesn't save the data to the main raw file)\n",
    "    '''\n",
    "    evoked_ar = epochs_ar.average()\n",
    "\n",
    "    [eog_indices,eog_scores]=ica.find_bads_eog(epochs_ar)\n",
    "\n",
    "    # plot ICs applied to the averaged EOG epochs, with EOG matches highlighted\n",
    "    ica.plot_sources(evoked_ar)\n",
    "    \n",
    "    ica.exclude.extend(eog_indices)\n",
    "    \n",
    "    reconst_raw = epochs_ar.copy()\n",
    "    ica.apply(inst=reconst_raw)\n",
    "    \n",
    "    evoked = reconst_raw.average()\n",
    "    evoked.plot()\n",
    "    \n",
    "    #set ERP to custom montage and plot topoplots\n",
    "    # evoked.set_montage(montage)\n",
    "    times = np.arange(0.0, 0.31, 0.05)\n",
    "    evoked.plot_topomap(times=times, ch_type='eeg')\n",
    "    \n",
    "    return evoked, reconst_raw\n",
    "        \n",
    "    \n",
    "def EOG_annot(raw2, ica):\n",
    "    '''\n",
    "    annotates data for when a bad blink occurs\n",
    "        - occurences (when EOG channels are correlated with specific electrode channel activity)\n",
    "    \n",
    "    Current Issues:\n",
    "        - overwriting stimulus annotations from original data\n",
    "\n",
    "    '''\n",
    "    # make a file, that has the file with the components rempved\n",
    "    #     then plot that clean data\n",
    "    #     0 is the ocular event\n",
    "    # you can reconstruct your, \n",
    "    # Review removing the ICA components on their own\n",
    "    #     project components back into the data\n",
    "    #         then subtract it fromthe data\n",
    "    #             have aw and then signal, then put them back\n",
    "    #             put the data back into original format, then you add it all together,\n",
    "    #             subtract tthe components you don't want\n",
    "                \n",
    "\n",
    "    eog_events = mne.preprocessing.find_eog_events(raw2)\n",
    "    onsets = eog_events[:, 0] / raw.info['sfreq']-0.25 \n",
    "    #take all the indices, divided by the sample rate, 2500hz,-0.25\n",
    "    durations = [0.6] * len( eog_events)\n",
    "    print(len(eog_events))\n",
    "    descriptions = ['bad blink'] * len( eog_events)\n",
    "    blink_annot = mne.Annotations(onsets, durations, descriptions,\n",
    "                                  orig_time=raw.info['meas_date'])\n",
    "    raw2.set_annotations(blink_annot)\n",
    "    eeg_picks = mne.pick_types(raw2.info, meg=False, eeg=True)\n",
    "    raw2.plot(events= eog_events, order=eeg_picks)\n",
    "    #raw.plot(events= eog_indices, order=eeg_picks)\n",
    "\n",
    "    return raw2\n",
    "\n",
    "\n",
    "def erp_blink(raw, montage):\n",
    "    '''\n",
    "    takes annotated data, wherever bad blink is listed\n",
    "    removes the annotation and then plots data\n",
    "\n",
    "    '''\n",
    "    #get events and their IDs from annotations in recorded eeg data\n",
    "    getData = mne.events_from_annotations(raw, event_id={'bad blink':998})\n",
    "\n",
    "\n",
    "    events = getData[0]\n",
    "    event_id = getData[1]\n",
    "\n",
    "    tmin = -0.5\n",
    "    tmax = 1.0\n",
    "    baseline = (-0.5, -0.1)\n",
    "    \n",
    "    #create epochs with baseline subtraction \n",
    "    epochs = mne.Epochs(raw, events=events, event_id=event_id, tmin=tmin, \n",
    "                        tmax=tmax, proj=True, baseline=baseline, preload=True,\n",
    "                        reject_by_annotation=True)\n",
    "    \n",
    "    #create ERP\n",
    "    evoked = epochs.average()\n",
    "    evoked.plot()\n",
    "    \n",
    "    #set ERP to custom montage and plot topoplots\n",
    "    # evoked.set_montage(montage)\n",
    "    times = np.arange(0.0, 0.31, 0.05)\n",
    "    evoked.plot_topomap(times=times, ch_type='eeg')\n",
    "    #cont = keyboard.read_key()\n",
    "    \n",
    "    \n",
    "def erp(raw, montage):  \n",
    "    '''\n",
    "    1. gets stimuli events from annotations in OG data set\n",
    "    2. separate the ID's\n",
    "    3. generate epochs\n",
    "    (slices of time from tmin to tmax based around stimuli occurences)\n",
    "    4. plot the average epoch (evoked)\n",
    "    5. set evoked plot to match the electrode placements\n",
    "    6. plot topographic map of the average epoch centered on the stimuli\n",
    "    \n",
    "    plots the ERP #5\n",
    "\n",
    "    '''\n",
    "    #get events and their IDs from annotations in recorded eeg data\n",
    "    getData = mne.events_from_annotations(raw, event_id={'Stimulus/S  1':1,\n",
    "                                                         'Stimulus/S  2':2,})\n",
    "    #                                                      'Stimulus/S  3':3,\n",
    "    #                                                      'Stimulus/S  4':4})\n",
    "    # print(getData[0])\n",
    "    # print(getData[1])\n",
    "    events = getData[0]\n",
    "    event_id = getData[1]\n",
    "\n",
    "    tmin = -0.5\n",
    "    tmax = 1.0\n",
    "    baseline = (-0.5, -0.1)\n",
    "    \n",
    "    #create epochs with baseline subtraction \n",
    "    epochs = mne.Epochs(raw, events=events, event_id=event_id, tmin=tmin, \n",
    "                        tmax=tmax, proj=True, baseline=baseline, preload=True,\n",
    "                        reject_by_annotation=False)\n",
    "    \n",
    "    #create ERP\n",
    "    evoked = epochs.average()\n",
    "    evoked.plot()\n",
    "    \n",
    "    #set ERP to custom montage and plot topoplots\n",
    "    # evoked.set_montage(montage)\n",
    "    times = np.arange(0.0, 0.31, 0.05)\n",
    "    evoked.plot_topomap(times=times, ch_type='eeg')\n",
    "    \n",
    "    return epochs\n",
    "    #cont = keyboard.read_key()\n",
    " \n",
    "    \n",
    "def ica_func(raw2):\n",
    "    '''\n",
    "    does ICA on the EEG data\n",
    "\n",
    "    '''\n",
    "    #define ica method, default fastica\n",
    "    ica = mne.preprocessing.ICA(method=\"fastica\")\n",
    "    \n",
    "    #fit data to ica\n",
    "    ica.fit(raw2)\n",
    "    \n",
    "\n",
    "    #plot ica components, set inst=data to make plots interactive\n",
    "    ica.plot_components(ch_type='eeg', inst=raw2) #, topomap_args={'data':raw})\n",
    "    \n",
    "    #cont = keyboard.read_key()\n",
    "    # reconst_raw = raw2.copy()\n",
    "    # ica.apply(inst=reconst_raw)\n",
    "    \n",
    "\n",
    "    # ica.plot_overlay(inst = reconst_raw)\n",
    "\n",
    "    return ica\n",
    "\n",
    "def autorej(epochs):\n",
    "    \n",
    "    n_interpolates = np.array([1, 4, 32])\n",
    "    consensus_percs = np.linspace(0, 1.0, 11)\n",
    "    ar = AutoReject(n_interpolates, consensus_percs, random_state = 26042021)\n",
    "    \n",
    "    ar.fit(epochs)\n",
    "    \n",
    "    epochs_ar, reject_log = ar.transform(epochs, return_log = True)\n",
    "    \n",
    "    #reject_log.plot_epochs(epochs, scalings = {\"eeg\":1e-4})\n",
    "    \n",
    "    evoked_ar = epochs_ar.average()\n",
    "    evoked_ar.plot()\n",
    "    \n",
    "    #set ERP to custom montage and plot topoplots\n",
    "    # evoked_ar.set_montage(montage)\n",
    "    times = np.arange(0.0, 0.31, 0.05)\n",
    "    evoked_ar.plot_topomap(times=times, ch_type='eeg')\n",
    "\n",
    "    return epochs_ar\n",
    "\n",
    "def ICA_auto_rej(epochs_ar):\n",
    "    '''\n",
    "    does ICA on the EEG data\n",
    "\n",
    "    '''\n",
    "    #define ica method, default fastica\n",
    "    ica = mne.preprocessing.ICA(method=\"fastica\")\n",
    "    \n",
    "    #fit data to ica\n",
    "    ica.fit(epochs_ar)\n",
    "    \n",
    "\n",
    "    #plot ica components, set inst=data to make plots interactive\n",
    "    ica.plot_components(ch_type='eeg', inst=epochs_ar) #, topomap_args={'data':raw})\n",
    "    \n",
    "    #cont = keyboard.read_key()\n",
    "    # reconst_raw = raw2.copy()\n",
    "    # ica.apply(inst=reconst_raw)\n",
    "    \n",
    "\n",
    "    # ica.plot_overlay(inst = reconst_raw)\n",
    "\n",
    "    return ica\n",
    "\n",
    "\n",
    "## COREGISTRATION STEPS\n",
    "\n",
    "def BEM(SUBJECTS_DIR):\n",
    "    surfaces = mne.bem.make_watershed_bem(subj, subjects_dir = SUBJECTS_DIR)\n",
    "    surfaces = mne.bem.make_scalp_surfaces(subj, subjects_dir = SUBJECTS_DIR)\n",
    "    mne.viz.plot_bem(subject = subj, subjects_dir = SUBJECTS_DIR, orientation = 'coronal', show = True, show_indices=True, mri = 'T1.mgz', show_orientation=True)\n",
    "    mne.bem.make_watershed_bem(subject=subj, subjects_dir=SUBJECTS_DIR, overwrite=True, volume='T1', atlas=False, gcaatlas=False, preflood=None, show=False, copy=False, T1=None, brainmask='ws.mgz', verbose=None)\n",
    "\n",
    "\n",
    "def covariance(reconst_raw):\n",
    "    '''\n",
    "    compute covariance from fresh raw data and epochs\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    noise_cov : TYPE\n",
    "        DESCRIPTION.\n",
    "    fig_cov : TYPE\n",
    "        DESCRIPTION.\n",
    "    fig_spectra : TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    noise_cov = mne.compute_covariance(reconst_raw, tmax = 0, method = ['shrunk', 'empirical'], rank = None, verbose = True)\n",
    "    noise_cov = mne.cov.regularize(noise_cov, raw2.info)\n",
    "    \n",
    "    fig_cov, fig_spectra = mne.viz.plot_cov(noise_cov, raw2.info)\n",
    "    \n",
    "    \n",
    "    evoked = reconst_raw.average()\n",
    "    evoked.plot_white(noise_cov, time_unit='s')\n",
    "    \n",
    "    \n",
    "    return noise_cov, fig_cov, fig_spectra, evoked\n",
    "\n",
    "def covariance_raw():\n",
    "    '''\n",
    "    compute covariance from fresh raw data and epochs\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    noise_cov : TYPE\n",
    "        DESCRIPTION.\n",
    "    fig_cov : TYPE\n",
    "        DESCRIPTION.\n",
    "    fig_spectra : TYPE\n",
    "        DESCRIPTION.\n",
    "\n",
    "    '''\n",
    "    montage, raw =  make_montage()\n",
    "    \n",
    "    getData = mne.events_from_annotations(raw, event_id={'Stimulus/S  1':1,\n",
    "                                                          'Stimulus/S  2':2,})\n",
    "\n",
    "    events = getData[0]\n",
    "    event_id = getData[1]\n",
    "\n",
    "    tmin = -0.5\n",
    "    tmax = 1.0\n",
    "    baseline = (-0.5, -0.1)\n",
    "    \n",
    "    #create epochs with baseline subtraction \n",
    "    epochs = mne.Epochs(raw, events=events, event_id=event_id, tmin=tmin, \n",
    "                        tmax=tmax, proj=True, baseline=baseline, preload=True,\n",
    "                        reject_by_annotation=False)\n",
    "    \n",
    "    # epochs.save(pathlib.Path('D:/Aidan/' + subj +'/Epochs') / 'epochs_for_source_epo.fif', overwrite = True) #to do co-registration, have to do file path so gui.coreg can take as input\n",
    "    epochs.save(pathlib.Path('/mnt/d/Aidan/' + subj +'/Epochs') / 'epochs_for_source_epo.fif', overwrite = True) #to do co-registration, have to do file path so gui.coreg can take as input\n",
    "    \n",
    "    #noise_cov = mne.compute_covariance(reconst_raw, tmax = 0, method = ['shrunk', 'empirical'], rank = None, verbose = True)\n",
    "    noise_cov = mne.compute_covariance(epochs, tmax = 0, method = ['shrunk', 'empirical'], rank = None, verbose = True)\n",
    "\n",
    "    noise_cov = mne.cov.regularize(noise_cov, epochs.info)\n",
    "    \n",
    "    fig_cov, fig_spectra = mne.viz.plot_cov(noise_cov, raw.info)\n",
    "    \n",
    "    \n",
    "    evoked = epochs.average()\n",
    "    evoked.plot_white(noise_cov, time_unit='s')\n",
    "    \n",
    "    \n",
    "    return noise_cov, fig_cov, fig_spectra, evoked\n",
    "\n",
    "\n",
    "def Co_register(SUBJECTS_DIR):\n",
    "    epochs_fname = pathlib.Path('/mnt/d/Aidan/' + subj +'/Epochs') / 'epochs_for_source_epo.fif'\n",
    "    data = pathlib.Path('/mnt/d/Aidan/' + subj + '/Digitization') / 'Captrak_Digitization.fif'\n",
    "    # epochs_fname = pathlib.Path('D:/Aidan/' + subj +'/Epochs') / 'epochs_for_source_epo.fif'\n",
    "    mne.gui.coregistration(subject = subj, subjects_dir=SUBJECTS_DIR, inst = data)\n",
    "\n",
    "#cd D:\\Aidan\n",
    "#%matplotlib qt\n",
    "\n",
    "def fsa_average(src, SUBJECTS_DIR):\n",
    "    home_path = '/mnt/d/Aidan'\n",
    "    data_path = join(home_path, 'data/')\n",
    "    save_dir_averages = data_path + 'Grand_Averages/Epochs'\n",
    "    save_dir = data_path + 'Grand_Averages'\n",
    "    fsa_average_file = SUBJECTS_DIR + '/fsaverage'\n",
    "    #stc_morph = mne.morph_data(subj, 'fsaverage', SUBJECTS_DIR, n_jobs=-1)\n",
    "    src_morph = mne.morph_source_spaces(src_from = src, subject_to='fsaverage', subjects_dir = SUBJECTS_DIR, surf = 'inflated')\n",
    "    stcs = mne.read_source_estimates(subject = subj,fname = save_dir)\n",
    "    \n",
    "    #fwd = mne.make_forward_solution(info, trans=trans, src=src_morph, bem=bem_sol, meg=False, eeg=True, mindist=5.0, n_jobs=1)\n",
    "    \n",
    "def evoked_grand_average(evoked_data_all):\n",
    "    \n",
    "    home_path = '/mnt/d/Aidan'\n",
    "    data_path = join(home_path, 'data/')\n",
    "    save_dir_averages = data_path + 'Grand_Averages/Epochs'\n",
    "    save_dir = data_path + 'Grand_Averages'\n",
    "    method = 'eLORETA'\n",
    "    \n",
    "def make_morphed_data_average(subject_list):\n",
    "    Inv_path = '/mnt/d/Aidan/Grand_Averages/Inv_op'\n",
    "    Evoked_path = '/mnt/d/Aidan/Grand_Averages/Epochs'\n",
    "    \n",
    "    subj = subject_list[0]\n",
    "    subj_inv_1 = mne.minimum_norm.read_inverse_operator(Inv_path + '/' + subj + 'inv.fif')\n",
    "    subj_ev_1 = mne.read_evokeds(Evoked_path + '/' + subj + '_raw_for_ave.fif')\n",
    "    stc = mne.minimum_norm.apply_inverse(subj_ev_1, subj_ev_1, method = 'eLORETA')\n",
    "    average_source_space = stc.copy() \n",
    "        \n",
    "    for i in range (1, len(subject_list)):\n",
    "        subj = subject_list[i]\n",
    "        subj_inv = mne.minimum_norm.read_inverse_operator(Inv_path + '/' + subj + 'inv.fif')\n",
    "        subj_ev = mne.read_evokeds(Evoked_path + '/' + subj + '_raw_for_Gaverage.fif')\n",
    "        stc = mne.minimum_norm.apply_inverse(subj_ev, subj_ev, method = 'eLORETA')\n",
    "        average_source_space.data += stc.data\n",
    "        \n",
    "    # subj1 = mne.minimum_norm.read_inverse_operator(Inv_path + 'E25' + 'inverse_op.fif')\n",
    "    # subj2 = mne.minimum_norm.read_inverse_operator(Inv_path + 'E29' + 'inverse_op.fif')\n",
    "    # subj3 = mne.minimum_norm.read_inverse_operator(Inv_path + 'E30' + 'inverse_op.fif')\n",
    "    # subj4 = mne.minimum_norm.read_inverse_operator(Inv_path + 'E31' + 'inverse_op.fif')\n",
    "    # subj1_ev = mne.read_evokeds(Evoked_path + 'E25' + '_raw_for_Gaverage.fif')\n",
    "    # mne.minimum_norm.apply_inverse(reconst_evoked, inv_morph, method = 'eLORETA')\n",
    "    \n",
    "    return average_source_space\n",
    "\n",
    "    \n",
    "def make_average_stc(subject_list):\n",
    "    sc_path = '/mnt/d/Aidan/Grand_Averages/Source_Estimates'\n",
    "    \n",
    "    subj = subject_list[0]\n",
    "    stc_lh_1 = mne.read_source_estimate(sc_path + '/' + subj + '_src-lh.stc', subj)\n",
    "    stc_rh_1 = mne.read_source_estimate(sc_path + '/' + subj + '_src-rh.stc', subj)\n",
    "    average_lh = stc_lh_1.copy() \n",
    "    average_rh = stc_rh_1.copy() \n",
    "\n",
    "    for i in range (1, len(subject_list)):\n",
    "        subj = subject_list[i]\n",
    "        stc_lh = mne.read_source_estimate(sc_path + '/' + subj + '_src-lh.stc', subj)\n",
    "        stc_rh = mne.read_source_estimate(sc_path + '/' + subj + '_src-rh.stc', subj)\n",
    "        average_lh += stc_lh_1\n",
    "        average_rh += stc_rh_1\n",
    "        \n",
    "    average_rh = average_rh/(len(subject_list))\n",
    "    average_lh = average_lh/(len(subject_list))\n",
    "\n",
    "    return average_lh, average_rh\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export FS_LICENSE='/usr/local/freesurfer/7.3.2/license.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "montage, raw = make_montage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw2 = set_data(raw, montage) #common average reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw2.set_montage(montage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = erp(raw2, montage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to change the plot if channels still need to be removed\n",
    "raw2.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUTOREJ ROUTE:\n",
    "epochs_ar = autorej(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_ar.save(pathlib.Path(D_Drive + os.sep + subj + os.sep + 'Digitization') / 'AR_epochs.fif', overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica = ICA_auto_rej(epochs_ar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.plot_sources(epochs_ar, show_scrollbars=True)     # to plot ICA vs time\n",
    "ica.plot_sources(raw2, show_scrollbars=True)     # to plot ICA vs time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reconst_evoked, reconst_raw = EOG_check_ar(epochs_ar, ica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica.exclude = [0,1,6,9,26,28,33,0,1,6,9,26,28,33] # MANUAL INPUT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw2.save(pathlib.Path(D_Drive + os.sep + subj + os.sep + 'Digitization') / 'Captrak_Digitization.fif', overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj = 'E9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in all the data  if skipping ahead\n",
    "reconst_raw = mne.io.read_raw_fif(D_Drive + os.sep + subj + os.sep + 'Digitization' + os.sep + 'Captrak_Digitization.fif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_cov = mne.read_cov(D_Drive + os.sep + subj + os.sep + 'Digitization' + os.sep + 'noise_cov.fif')\n",
    "#subj = 'fsaverage'\n",
    "#use subj = 'fsaverage' if subject doesn't have usable MRI data\n",
    "bem_sol = mne.read_bem_solution(D_Drive + os.sep + subj + os.sep + 'Digitization' + os.sep + subj+ '_bem.fif')\n",
    "#OR LOAD IN THE SOURCE SPACE\n",
    "# src= mne.read_source_spaces('/mnt/d/Aidan/Patient_SRC/' + subj+ '-src.fif')\n",
    "src = mne.read_source_spaces(D_Drive + os.sep + 'Patient_SRC' + os.sep + subj + '-src.fif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_cov, fig_cov, fig_spectra, evoked = covariance(reconst_raw)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reconst_raw.save(D_Drive + os.sep + subj + os.sep + 'Digitization' + os.sep + 'Captrak_Digitization.fif', overwrite = True)\n",
    "mne.write_cov(D_Drive + os.sep + subj + os.sep + 'Digitization' + os.sep + 'noise_cov.fif', noise_cov, overwrite = True)\n",
    "mne.write_evokeds(D_Drive + os.sep + 'Grand_Averages' + os.sep + 'Epochs' + subj + '_raw_for_ave.fif', reconst_evoked) #to do co-registration, have to do file path so gui.coreg can take as input\n",
    "ica.save(pathlib.Path(D_Drive + os.sep + subj + os.sep + 'Digitization') / 'ica.fif', overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = D_Drive + os.sep + subj + os.sep + 'Digitization'\n",
    "data = pathlib.Path(data_path) / 'Captrak_Digitization.fif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = pathlib.Path(data_path)/ (subj + '-trans.fif')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # read in that data from the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a. use baseline evoked\n",
    "info = mne.io.read_info(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#b. or use the evoked data you have already generated\n",
    "info = reconst_evoked.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#c. or read in cleaned evoked\n",
    "reconst_evoked = mne.read_evokeds(D_Drive + os.sep + 'Grand_Averages/Epochs/' + subj + '_raw_for_ave.fif')\n",
    "info = reconst_evoked[0].info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if bem isn't generated run this - if in vscode please use your terminal to generate this file\n",
    "# mne watershed_bem -s subj --overwrite T\n",
    "mne.bem.make_watershed_bem(subject=subj, subjects_dir=SUBJECTS_DIR, overwrite=True, volume='T1')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mne.make_bem_model(subject=subj, subjects_dir=SUBJECTS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mne.viz.plot_bem(subject = subj, subjects_dir = SUBJECTS_DIR, orientation = 'coronal')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bem_sol=mne.make_bem_solution(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. compute your space (2D) - oct4, oct5 or oct6 are recommended \n",
    "# (these determine the number of sources), you can also change the\n",
    "#  'conductance' (then save to the correct folder of your choice) \n",
    "# subj = 'E2'\n",
    "src = mne.setup_source_space(subject='fsaverage', spacing='oct5', subjects_dir = SUBJECTS_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the src\n",
    "src.save(pathlib.Path(SUBJECTS_DIR + os.sep + subj + os.sep + 'bem') / '-src.fif',overwrite = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fwd = mne.make_forward_solution(info, trans=trans, src=src, bem=bem_sol, meg=False, eeg=True, n_jobs=1) # mindist=5.0,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv = mne.minimum_norm.make_inverse_operator(info, fwd, noise_cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stc = mne.minimum_norm.apply_inverse(reconst_evoked[0], inv, method = 'eLORETA', lambda2 = 0.1111111111111111111111111111111)\n",
    "# if reconst_evoked loaded in use this one\n",
    "# stc = mne.minimum_norm.apply_inverse(reconst_evoked[0], inv, method = 'eLORETA', lambda2 = 0.1111111111111111111111111111111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stc.plot(hemi = 'split',smoothing_steps = 5, surface = 'pial')  # hemi = 'both'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### FOR MORPHING INTO FSAVERGE SOURCE SPACE\n",
    "fsa_src = mne.read_source_spaces(SUBJECTS_DIR + '/fsaverage/bem/fsaverage-ico-5-src.fif')\n",
    "stc_morph_fsa = mne.compute_source_morph(src, subject_from=subj, subject_to='fsaverage', subjects_dir=SUBJECTS_DIR, src_to=fsa_src)\n",
    "stc_final = stc_morph_fsa.apply(stc) #stc_current\n",
    "stc_final.plot(hemi = 'split',smoothing_steps = 5, surface = 'pial')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stc_final.save(pathlib.Path(D_Drive + os.sep + subj) / 'fsaverage_base_stc', overwrite = True)\n",
    "stc.save(pathlib.Path(D_Drive + os.sep + subj) / 'base_stc', overwrite = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIMULATE SOURCE ACTIVITY - FINAL\n",
    "\n",
    "# RUN FULL PIPELINE THEN\n",
    "\n",
    "import os.path as op\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import mne\n",
    "from mne.datasets import sample\n",
    "\n",
    "from mne.minimum_norm import read_inverse_operator, apply_inverse\n",
    "from mne.simulation import simulate_stc, simulate_evoked\n",
    "\n",
    "labels = mne.read_labels_from_annot(subj, subjects_dir=SUBJECTS_DIR)\n",
    "label_names = [label.name for label in labels]\n",
    "n_labels = len(labels)\n",
    "\n",
    "nave = 200\n",
    "T = 150\n",
    "times = np.linspace(-0.5, 1, T)\n",
    "dt = times[1] - times[0]\n",
    "\n",
    "seed = 42\n",
    "lambda2 = 0.1111\n",
    "signal = np.zeros((n_labels, T))\n",
    "idx_lh = label_names.index('superiortemporal-lh')\n",
    "signal[idx_lh, :] = 1e-7 * np.sin(7 * 2 * np.pi * times)\n",
    "idx_rh = label_names.index('superiortemporal-rh')\n",
    "signal[idx_rh, :] = 1e-7 * np.sin(7 * 2 * np.pi * times)\n",
    "\n",
    "hemi_to_ind = {'lh': 0, 'rh': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i, label in enumerate(labels):\n",
    "    # The `center_of_mass` function needs labels to have values.\n",
    "    labels[i].values.fill(1.)\n",
    "\n",
    "    # Restrict the eligible vertices to be those on the surface under\n",
    "    # consideration and within the label.\n",
    "    surf_vertices = fwd['src'][hemi_to_ind[label.hemi]]['vertno']\n",
    "    restrict_verts = np.intersect1d(surf_vertices, label.vertices)\n",
    "    if (np.size(restrict_verts) == 0):\n",
    "         used_verts = labels[i].get_vertices_used(surf_vertices)\n",
    "         if np.size(used_verts) == 0:\n",
    "            used_verts = np.arange(10242)\n",
    "         com = labels[i].center_of_mass(subjects_dir=SUBJECTS_DIR, \n",
    "         restrict_vertices=used_verts, surf='white') \n",
    "    else: \n",
    "        com = labels[i].center_of_mass(subjects_dir=SUBJECTS_DIR,\n",
    "         restrict_vertices=restrict_verts, surf='white')\n",
    "\n",
    "        # Convert the center of vertex index from surface vertex list to Label's\n",
    "        # vertex list.\n",
    "        cent_idx = np.where(label.vertices == com)[0][0]\n",
    "        if i  == idx_lh:\n",
    "            cent_vertice_lh = label.vertices[cent_idx]\n",
    "            cent_idx_lh = np.where(surf_vertices == cent_vertice_lh)[0][0]\n",
    "            #cent_idx_lh_temp = cent_idx\n",
    "            #cent_idx_lh = surf_vertices[tmp] \n",
    "        if i == idx_rh:\n",
    "            cent_vertice_rh = label.vertices[cent_idx]\n",
    "            cent_idx_rh = np.where(surf_vertices == cent_vertice_rh)[0][0]\n",
    "            #cent_idx_rh_temp = cent_idx\n",
    "            #print(label)\n",
    "            #print(label.vertices[cent_idx_rh])\n",
    "            #print(cent_idx)\n",
    "            #print(cent_vertice_rh)\n",
    "            #print(cent_idx_rh)\n",
    "            #print(surf_vertices[cent_idx_rh])\n",
    "            #cent_idx_rh = surf_vertices[tmp] \n",
    "        # Create a mask with 1 at center vertex and zeros elsewhere.\n",
    "        labels[i].values.fill(0.)\n",
    "        labels[i].values[cent_idx] = 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "for i, label in enumerate(labels):\n",
    "    if i  == idx_lh:\n",
    "                # The `center_of_mass` function needs labels to have values.\n",
    "        labels[i].values.fill(1.)\n",
    "\n",
    "        # Restrict the eligible vertices to be those on the surface under\n",
    "        # consideration and within the label.\n",
    "        surf_vertices = fwd2['src'][hemi_to_ind[label.hemi]]['vertno']\n",
    "        restrict_verts = np.intersect1d(surf_vertices, label.vertices)\n",
    "        com = labels[i].center_of_mass(subjects_dir=SUBJECTS_DIR,\n",
    "        restrict_vertices=restrict_verts, surf='white')\n",
    "\n",
    "        # Convert the center of vertex index from surface vertex list to Label's\n",
    "        # vertex list.\n",
    "        cent_idx = np.where(label.vertices == com)[0][0]\n",
    "\n",
    "\n",
    "        cent_vertice_lh = label.vertices[cent_idx]\n",
    "        cent_idx_lh = np.where(surf_vertices == cent_vertice_lh)[0][0]\n",
    "        #cent_idx_lh_temp = cent_idx\n",
    "        #cent_idx_lh = surf_vertices[tmp] \n",
    "\n",
    "        # Create a mask with 1 at center vertex and zeros elsewhere.\n",
    "        labels[i].values.fill(0.)\n",
    "        labels[i].values[cent_idx] = 1.\n",
    "\n",
    "    if i == idx_rh:\n",
    "                        # The `center_of_mass` function needs labels to have values.\n",
    "        labels[i].values.fill(1.)\n",
    "\n",
    "        # Restrict the eligible vertices to be those on the surface under\n",
    "        # consideration and within the label.\n",
    "        surf_vertices = fwd2['src'][hemi_to_ind[label.hemi]]['vertno']\n",
    "        restrict_verts = np.intersect1d(surf_vertices, label.vertices)\n",
    "        com = labels[i].center_of_mass(subjects_dir=SUBJECTS_DIR,\n",
    "        restrict_vertices=restrict_verts, surf='white')\n",
    "\n",
    "        # Convert the center of vertex index from surface vertex list to Label's\n",
    "        # vertex list.\n",
    "        cent_idx = np.where(label.vertices == com)[0][0]\n",
    "\n",
    "        \n",
    "        cent_vertice_rh = label.vertices[cent_idx]\n",
    "        cent_idx_rh = np.where(surf_vertices == cent_vertice_rh)[0][0]\n",
    "        #cent_idx_rh_temp = cent_idx\n",
    "        #print(label)\n",
    "        #print(label.vertices[cent_idx_rh])\n",
    "        #print(cent_idx)\n",
    "        #print(cent_vertice_rh)\n",
    "        #print(cent_idx_rh)\n",
    "        #print(surf_vertices[cent_idx_rh])\n",
    "        #cent_idx_rh = surf_vertices[tmp] \n",
    "\n",
    "        # Create a mask with 1 at center vertex and zeros elsewhere.\n",
    "        labels[i].values.fill(0.)\n",
    "        labels[i].values[cent_idx] = 1.\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stc_gen = simulate_stc(fwd['src'], labels, signal, times[0], dt,\n",
    "                       value_fun=lambda x: x)\n",
    "\n",
    "##-- plot\n",
    "\n",
    "kwargs = dict(subjects_dir=SUBJECTS_DIR, hemi='split', smoothing_steps=5,\n",
    "              time_unit='s', initial_time=0.05, size=1200,\n",
    "              views=['lat', 'med'])\n",
    "clim = dict(kind='value', pos_lims=[1e-9, 1e-8, 1e-7])\n",
    "brain_gen = stc_gen.plot(clim=clim, **kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stc_gen.save(pathlib.Path(D_Drive + os.sep + subj) / 'simulated_stc', overwrite = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = mne.read_epochs(pathlib.Path(D_Drive + os.sep + subj + os.sep + 'Digitization'+ os.sep +'AR_epochs.fif'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs2 = epochs\n",
    "epochs2.info['bads'] = []\n",
    "cov = mne.compute_covariance(epochs2, tmin=None, tmax=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#-- ISSUE WITH INVERSE OPERATOR** --> \n",
    "fwd2 = mne.convert_forward_solution(fwd, surf_ori=True,\n",
    "                             force_fixed=True, copy=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv = mne.minimum_norm.make_inverse_operator(\n",
    "    info=info, forward=fwd2, noise_cov=cov, loose=0.,\n",
    "    depth=None)\n",
    "\n",
    "inv_path = D_Drive + os.sep + 'Grand_Averages' + os.sep + 'Inv_op'\n",
    "mne.minimum_norm.write_inverse_operator(fname = inv_path + os.sep + subj +'_inv.fif',inv=inv, overwrite = True)\n",
    "# this one reloaded\n",
    "inv_reloaded = mne.minimum_norm.read_inverse_operator(inv_path + os.sep +  subj +'_inv.fif')\n",
    "\n",
    "##--simulate source space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evoked_gen = simulate_evoked(fwd2, stc_gen, info, cov, nave,\n",
    "                             random_state=seed)\n",
    "\n",
    "# Map the simulated sensor-space data to source-space using the inverse\n",
    "# operator.\n",
    "stc_inv = apply_inverse(evoked_gen, inv_reloaded, lambda2, method='eLORETA')\n",
    "stc_inv.plot(surface = 'inflated',hemi = 'split', smoothing_steps = 5)\n",
    "stc_inv.save(pathlib.Path(D_Drive + os.sep + subj) / 'simulated_stc', overwrite = True)\n",
    "\n",
    "\n",
    "# --resolution MATRIX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### RESOUTION MATRIX ----- PSF CTF\n",
    "\n",
    "from mne.minimum_norm import (make_inverse_resolution_matrix, get_cross_talk,\n",
    "                              get_point_spread)\n",
    "\n",
    "rm_lor = make_inverse_resolution_matrix(fwd2, inv_reloaded,\n",
    "                                        method='eLORETA', lambda2=lambda2)\n",
    "                                        \n",
    "\n",
    "# get PSF and CTF for sLORETA at one vertex\n",
    "#cent_idx_rh_idx = [cent_idx_rh]\n",
    "#cent_idx_lh_idx = [cent_idx_lh]\n",
    "\n",
    "#cent_idx_lh = [labels[61]]\n",
    "#cent_idx_rh = [labels[62]]\n",
    "\n",
    "cent_idx_rh_idx = [cent_idx_rh]\n",
    "cent_idx_lh_idx = [cent_idx_lh]\n",
    "\n",
    "stc_psf_lh = get_point_spread(rm_lor, fwd2['src'], cent_idx_lh_idx, norm=True)\n",
    "\n",
    "stc_ctf_lh = get_cross_talk(rm_lor, fwd2['src'], cent_idx_lh_idx, norm=True)\n",
    "\n",
    "stc_psf_rh = get_point_spread(rm_lor, fwd2['src'], cent_idx_rh_idx, norm=True)\n",
    "\n",
    "stc_ctf_rh = get_cross_talk(rm_lor, fwd2['src'], cent_idx_rh_idx, norm=True)\n",
    "\n",
    "#----- psf setup -----\n",
    "\n",
    "# Which vertex corresponds to selected source\n",
    "vertno_lh = fwd2['src'][0]['vertno'] #vertices in left hemisphere\n",
    "verttrue_lh = [vertno_lh[cent_idx_lh_idx[0]]]  # just one vertex (at center of label)\n",
    "\n",
    "# find vertices with maxima in PSF and CTF\n",
    "vert_max_psf_lh = vertno_lh[stc_psf_lh.data.argmax()]\n",
    "vert_max_ctf_lh = vertno_lh[stc_ctf_lh.data.argmax()]\n",
    "\n",
    "# Which vertex corresponds to selected source in right hemisphere\n",
    "vertno_rh = fwd2['src'][1]['vertno']  \n",
    "verttrue_rh = [vertno_rh[cent_idx_rh_idx[0]]]  # just one vertex (center of label)\n",
    "\n",
    "# find vertices with maxima in PSF and CTF\n",
    "vert_max_psf_rh = vertno_rh[stc_psf_rh.data.argmax()]\n",
    "if stc_ctf_rh.data.argmax() > 1026:\n",
    "\tta = stc_ctf_rh.data\n",
    "\tta = ta[0:1026]\n",
    "\tvert_max_ctf_rh = vertno_rh[ta.argmax()]\n",
    "else:\n",
    "\tvert_max_ctf_rh = vertno_rh[stc_ctf_rh.data.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----LH - PSF -----\n",
    "\n",
    "brain_psf = stc_psf_lh.plot(subj, 'inflated', 'lh', smoothing_steps = 5, subjects_dir=SUBJECTS_DIR)\n",
    "brain_psf.show_view('ventral')\n",
    "brain_psf.add_text(0.1, 0.9, 'eLORETA PSF lh', 'title', font_size=16)\n",
    "\n",
    "# True source location for PSF LEFT HEMI\n",
    "brain_psf.add_foci(verttrue_lh, coords_as_verts=True, scale_factor=1., hemi='lh',\n",
    "                   color='green')\n",
    "\n",
    "# Maximum of PSF LEFT HEMI\n",
    "brain_psf.add_foci(vert_max_psf_lh, coords_as_verts=True, scale_factor=1.,\n",
    "                  hemi='lh', color='black')\n",
    "\n",
    "# -----RH - PSF -----\n",
    "\n",
    "brain_psf_rh = stc_psf_rh.plot(subj, 'inflated', 'rh', smoothing_steps = 5, subjects_dir=SUBJECTS_DIR)\n",
    "brain_psf_rh.show_view('ventral')\n",
    "brain_psf_rh.add_text(0.1, 0.9, 'eLORETA PSF rh', 'title', font_size=16)\n",
    "\n",
    "# True source location for PSF\n",
    "brain_psf_rh.add_foci(verttrue_rh, coords_as_verts=True, scale_factor=1., hemi='rh',\n",
    "                   color='green')\n",
    "\n",
    "# Maximum of PSF \n",
    "brain_psf_rh.add_foci(vert_max_psf_rh, coords_as_verts=True, scale_factor=1.,\n",
    "                   hemi='rh', color='black')\n",
    "\n",
    "#---- ctf ------\n",
    "\n",
    "brain_ctf = stc_ctf_lh.plot(subj, 'inflated', 'lh', smoothing_steps = 5, subjects_dir=SUBJECTS_DIR)\n",
    "brain_ctf.add_text(0.1, 0.9, 'eLORETA CTF lh', 'title', font_size=16)\n",
    "brain_ctf.show_view('ventral')\n",
    "\n",
    "brain_ctf_rh = stc_ctf_rh.plot(subj, 'inflated', 'rh', smoothing_steps = 5, subjects_dir=SUBJECTS_DIR)\n",
    "brain_ctf_rh.add_text(0.1, 0.9, 'eLORETA CTF rh', 'title', font_size=16)\n",
    "brain_ctf_rh.show_view('ventral')\n",
    "\n",
    "# -----LH - CTF -----\n",
    "brain_ctf.add_foci(verttrue_lh, coords_as_verts=True, scale_factor=1., hemi='lh',\n",
    "                   color='green')\n",
    "\n",
    "# Maximum of CTF\n",
    "brain_ctf.add_foci(vert_max_ctf_lh, coords_as_verts=True, scale_factor=1.,\n",
    "                   hemi='lh', color='black')\n",
    "\n",
    "# -----RH - CTF -----\n",
    "brain_ctf_rh.add_foci(verttrue_rh, coords_as_verts=True, scale_factor=1., hemi='rh',\n",
    "                   color='green')\n",
    "\n",
    "# Maximum of CTF - RH\n",
    "brain_ctf_rh.add_foci(vert_max_ctf_rh, coords_as_verts=True, scale_factor=1.,\n",
    "                   hemi='rh', color='black')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#------------------------ COMPARING DIFFERENT METHODS -------------------\n",
    "\n",
    "from mne.minimum_norm import make_inverse_resolution_matrix\n",
    "from mne.minimum_norm import resolution_metrics\n",
    "\n",
    "## ------------------------PSF-----------------------\n",
    "\n",
    "# -- MNE --\n",
    "\n",
    "rm_mne = make_inverse_resolution_matrix(fwd2, inv_reloaded,\n",
    "                                        method='MNE', lambda2=lambda2)\n",
    "ple_mne_psf = resolution_metrics(rm_mne, inv_reloaded['src'],\n",
    "                                 function='psf', metric='peak_err')\n",
    "sd_mne_psf = resolution_metrics(rm_mne, inv_reloaded['src'],\n",
    "                                function='psf', metric='sd_ext')\n",
    "\n",
    "tmp_sd_M = sd_mne_psf.to_data_frame()\n",
    "tmp_ple_M = ple_mne_psf.to_data_frame()\n",
    "tmp_sd_M.to_csv(D_Drive + os.sep + 'Grand_Averages' + os.sep + 'SD_PSF' + os.sep + 'sd_mne_psf_' + subj + '.csv')\n",
    "tmp_ple_M.to_csv(D_Drive + os.sep + 'Grand_Averages' + os.sep + 'PLE_PSF' + os.sep + 'ple_mne_psf' + subj + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- dSPM --\n",
    "\n",
    "rm_dspm = make_inverse_resolution_matrix(fwd2, inv_reloaded,\n",
    "                                        method='dSPM', lambda2=lambda2)\n",
    "ple_dspm_psf = resolution_metrics(rm_dspm, inv_reloaded['src'],\n",
    "                                 function='psf', metric='peak_err')\n",
    "sd_dspm_psf = resolution_metrics(rm_dspm, inv_reloaded['src'],\n",
    "                                function='psf', metric='sd_ext')\n",
    "tmp_sd_ds = sd_dspm_psf.to_data_frame()\n",
    "tmp_ple_ds = ple_dspm_psf.to_data_frame()\n",
    "tmp_sd_ds.to_csv(D_Drive + os.sep + 'Grand_Averages' + os.sep + 'SD_PSF' + os.sep + 'sd_dspm_psf_' + subj + '.csv')\n",
    "tmp_ple_ds.to_csv(D_Drive + os.sep + 'Grand_Averages' + os.sep + 'PLE_PSF' + os.sep + 'ple_dspm_psf' + subj + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- sLORETA --\n",
    "\n",
    "rm_sLORETA = make_inverse_resolution_matrix(fwd2, inv_reloaded,\n",
    "                                        method='sLORETA', lambda2=lambda2)\n",
    "ple_sLORETA_psf = resolution_metrics(rm_sLORETA, inv_reloaded['src'],\n",
    "                                 function='psf', metric='peak_err')\n",
    "sd_sLORETA_psf = resolution_metrics(rm_sLORETA, inv_reloaded['src'],\n",
    "                                function='psf', metric='sd_ext')\n",
    "\n",
    "tmp_sd_s = sd_sLORETA_psf.to_data_frame()\n",
    "tmp_ple_s = ple_sLORETA_psf.to_data_frame()\n",
    "tmp_sd_s.to_csv(D_Drive + os.sep + 'Grand_Averages' + os.sep + 'SD_PSF' + os.sep + 'sd_sLORETA_psf_' + subj + '.csv')\n",
    "tmp_ple_s.to_csv(D_Drive + os.sep + 'Grand_Averages' + os.sep + 'PLE_PSF' + os.sep + 'ple_sLORETA_psf' + subj + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- eLORETA --\n",
    "\n",
    "ple_eLORETA_psf = resolution_metrics(rm_lor, inv_reloaded['src'],\n",
    "                                 function='psf', metric='peak_err')\n",
    "sd_eLORETA_psf = resolution_metrics(rm_lor, inv_reloaded['src'],\n",
    "                                function='psf', metric='sd_ext')\n",
    "\n",
    "tmp_sd_e = sd_eLORETA_psf.to_data_frame()\n",
    "tmp_ple_e = ple_eLORETA_psf.to_data_frame()\n",
    "tmp_sd_e.to_csv(D_Drive + os.sep + 'Grand_Averages' + os.sep + 'SD_PSF' + os.sep + 'sd_eLORETA_psf_' + subj + '.csv')\n",
    "tmp_ple_e.to_csv(D_Drive + os.sep + 'Grand_Averages' + os.sep + 'PLE_PSF' + os.sep + 'ple_eLORETA_psf' + subj + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------- CTF -------------------\n",
    "\n",
    "# -- MNE --\n",
    "\n",
    "ple_mne_ctf = resolution_metrics(rm_mne, inv_reloaded['src'],\n",
    "                                 function='ctf', metric='peak_err')\n",
    "sd_mne_ctf = resolution_metrics(rm_mne, inv_reloaded['src'],\n",
    "                                function='ctf', metric='sd_ext')\n",
    "\n",
    "ctmp_sd_M = sd_mne_ctf.to_data_frame()\n",
    "ctmp_ple_M = ple_mne_ctf.to_data_frame()\n",
    "ctmp_sd_M.to_csv(D_Drive + os.sep + 'Grand_Averages' + os.sep + 'SD_CTF' + os.sep + 'sd_mne_ctf_' + subj + '.csv')\n",
    "ctmp_ple_M.to_csv(D_Drive + os.sep + 'Grand_Averages' + os.sep + 'PLE_CTF' + os.sep + 'ple_mne_ctf' + subj + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- dSPM --\n",
    "\n",
    "ple_dspm_ctf = resolution_metrics(rm_dspm, inv_reloaded['src'],\n",
    "                                 function='ctf', metric='peak_err')\n",
    "sd_dspm_ctf = resolution_metrics(rm_dspm, inv_reloaded['src'],\n",
    "                                function='ctf', metric='sd_ext')\n",
    "\n",
    "ctmp_sd_ds = sd_dspm_ctf.to_data_frame()\n",
    "ctmp_ple_ds = ple_dspm_ctf.to_data_frame()\n",
    "ctmp_sd_ds.to_csv(D_Drive + os.sep + 'Grand_Averages' + os.sep + 'SD_CTF' + os.sep + 'sd_ds_ctf_' + subj + '.csv')\n",
    "ctmp_ple_ds.to_csv(D_Drive + os.sep + 'Grand_Averages' + os.sep + 'PLE_CTF' + os.sep + 'ple_ds_ctf' + subj + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- sLORETA --\n",
    "\n",
    "ple_sLORETA_ctf = resolution_metrics(rm_sLORETA, inv_reloaded['src'],\n",
    "                                 function='ctf', metric='peak_err')\n",
    "sd_sLORETA_ctf = resolution_metrics(rm_sLORETA, inv_reloaded['src'],\n",
    "                                function='ctf', metric='sd_ext')\n",
    "\n",
    "ctmp_sd_s = sd_sLORETA_ctf.to_data_frame()\n",
    "ctmp_ple_s = ple_sLORETA_ctf.to_data_frame()\n",
    "ctmp_sd_s.to_csv(D_Drive + os.sep + 'Grand_Averages' + os.sep + 'SD_CTF' + os.sep + 'sd_sLORETA_ctf_' + subj + '.csv')\n",
    "ctmp_ple_s.to_csv(D_Drive + os.sep + 'Grand_Averages' + os.sep + 'PLE_CTF' + os.sep + 'ple_sLORETA_ctf' + subj + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- eLORETA --\n",
    "\n",
    "ple_eLORETA_ctf = resolution_metrics(rm_lor, inv_reloaded['src'],\n",
    "                                 function='ctf', metric='peak_err')\n",
    "sd_eLORETA_ctf = resolution_metrics(rm_lor, inv_reloaded['src'],\n",
    "                                function='ctf', metric='sd_ext')\n",
    "\n",
    "ctmp_sd_e = sd_sLORETA_ctf.to_data_frame()\n",
    "ctmp_ple_e = ple_sLORETA_ctf.to_data_frame()\n",
    "ctmp_sd_e.to_csv(D_Drive + os.sep + 'Grand_Averages' + os.sep + 'SD_CTF' + os.sep + 'sd_eLORETA_ctf_' + subj + '.csv')\n",
    "ctmp_ple_e.to_csv(D_Drive + os.sep + 'Grand_Averages' + os.sep + 'PLE_CTF' + os.sep + 'ple_eLORETA_ctf' + subj + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -- VISUALIZING --\n",
    "\n",
    "\n",
    "## -- MNE, dSPM, sLORETA, eLORETA PLE --\n",
    "brain_ple_mne = ple_mne_psf.plot(subj, 'inflated', 'split',\n",
    "                                 subjects_dir=SUBJECTS_DIR, figure=1,\n",
    "                                 clim=dict(kind='value', lims=(0, 2, 4)))\n",
    "brain_ple_mne.add_text(0.1, 0.9, 'PLE MNE', 'title', font_size=16)\n",
    "\n",
    "brain_ple_dspm = ple_dspm_psf.plot(subj, 'inflated', 'split',\n",
    "                                   subjects_dir=SUBJECTS_DIR, figure=2,\n",
    "                                   clim=dict(kind='value', lims=(0, 2, 4)))\n",
    "brain_ple_dspm.add_text(0.1, 0.9, 'PLE dSPM', 'title', font_size=16)\n",
    "\n",
    "brain_ple_sLORETA = ple_sLORETA_psf.plot(subj, 'inflated', 'split',\n",
    "                                   subjects_dir=SUBJECTS_DIR, figure=3,\n",
    "                                   clim=dict(kind='value', lims=(0, 2, 4)))\n",
    "brain_ple_sLORETA.add_text(0.1, 0.9, 'PLE sLORETA', 'title', font_size=16)\n",
    "\n",
    "brain_ple_eLORETA = ple_eLORETA_psf.plot(subj, 'inflated', 'split',\n",
    "                                   subjects_dir=SUBJECTS_DIR, figure=4,\n",
    "                                   clim=dict(kind='value', lims=(0, 2, 4)))\n",
    "brain_ple_eLORETA.add_text(0.1, 0.9, 'PLE eLORETA', 'title', font_size=16)\n",
    "\n",
    "## -- MNE, dSPM, sLORETA, eLORETA SD --\n",
    "\n",
    "brain_sd_mne = sd_mne_psf.plot(subj, 'inflated', 'split',\n",
    "                               subjects_dir=SUBJECTS_DIR, figure=5,\n",
    "                               clim=dict(kind='value', lims=(0, 2, 4)))\n",
    "brain_sd_mne.add_text(0.1, 0.9, 'SD MNE', 'title', font_size=16)\n",
    "\n",
    "brain_sd_dspm = sd_dspm_psf.plot(subj, 'inflated', 'split',\n",
    "                                 subjects_dir=SUBJECTS_DIR, figure=6,\n",
    "                                 clim=dict(kind='value', lims=(0, 2, 4)))\n",
    "brain_sd_dspm.add_text(0.1, 0.9, 'SD dSPM', 'title', font_size=16)\n",
    "\n",
    "brain_sd_sLORETA = sd_sLORETA_psf.plot(subj, 'inflated', 'split',\n",
    "                                 subjects_dir=SUBJECTS_DIR, figure=7,\n",
    "                                 clim=dict(kind='value', lims=(0, 2, 4)))\n",
    "brain_sd_sLORETA.add_text(0.1, 0.9, 'SD sLORETA', 'title', font_size=16)\n",
    "\n",
    "brain_sd_eLORETA = ple_eLORETA_psf.plot(subj, 'inflated', 'split',\n",
    "                                   subjects_dir=SUBJECTS_DIR, figure=8,\n",
    "                                   clim=dict(kind='value', lims=(0, 2, 4)))\n",
    "brain_sd_eLORETA.add_text(0.1, 0.9, 'SD eLORETA', 'title', font_size=16)\n",
    "\n",
    "## -- MNE and dSPM difference for sd and PLE --\n",
    "\n",
    "diff_ple = ple_mne_psf - ple_dspm_psf\n",
    "\n",
    "brain_ple_diff = diff_ple.plot(subj, 'inflated', 'split',\n",
    "                               subjects_dir=SUBJECTS_DIR, figure=9,\n",
    "                               clim=dict(kind='value', pos_lims=(0., 1., 2.)))\n",
    "brain_ple_diff.add_text(0.1, 0.9, 'PLE MNE-dSPM', 'title', font_size=16)\n",
    "\n",
    "diff_sd = sd_mne_psf - sd_dspm_psf\n",
    "\n",
    "brain_sd_diff = diff_sd.plot(subj, 'inflated', 'split',\n",
    "                             subjects_dir=SUBJECTS_DIR, figure=10,\n",
    "                             clim=dict(kind='value', pos_lims=(0., 1., 2.)))\n",
    "brain_sd_diff.add_text(0.1, 0.9, 'SD MNE-dSPM', 'title', font_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################################################\n",
    "## GETTING ANATOMICAL PLE ----------------------\n",
    "\n",
    "\n",
    "stc_psf_lh_mne = get_point_spread(rm_mne, fwd2['src'], cent_idx_lh_idx, norm=True)\n",
    "\n",
    "stc_ctf_lh_mne = get_cross_talk(rm_mne, fwd2['src'], cent_idx_lh_idx, norm=True)\n",
    "\n",
    "stc_psf_rh_mne = get_point_spread(rm_mne, fwd2['src'], cent_idx_rh_idx, norm=True)\n",
    "\n",
    "stc_ctf_rh_mne = get_cross_talk(rm_mne, fwd2['src'], cent_idx_rh_idx, norm=True)\n",
    "\n",
    "#--\n",
    "stc_psf_lh_dspm = get_point_spread(rm_dspm, fwd2['src'], cent_idx_lh_idx, norm=True)\n",
    "\n",
    "stc_ctf_lh_dspm = get_cross_talk(rm_dspm, fwd2['src'], cent_idx_lh_idx, norm=True)\n",
    "\n",
    "stc_psf_rh_dspm = get_point_spread(rm_dspm, fwd2['src'], cent_idx_rh_idx, norm=True)\n",
    "\n",
    "stc_ctf_rh_dspm = get_cross_talk(rm_dspm, fwd2['src'], cent_idx_rh_idx, norm=True)\n",
    "\n",
    "#--\n",
    "stc_psf_lh_sLORETA = get_point_spread(rm_sLORETA, fwd2['src'], cent_idx_lh_idx, norm=True)\n",
    "\n",
    "stc_ctf_lh_sLORETA = get_cross_talk(rm_sLORETA, fwd2['src'], cent_idx_lh_idx, norm=True)\n",
    "\n",
    "stc_psf_rh_sLORETA = get_point_spread(rm_sLORETA, fwd2['src'], cent_idx_rh_idx, norm=True)\n",
    "\n",
    "stc_ctf_rh_sLORETA = get_cross_talk(rm_sLORETA, fwd2['src'], cent_idx_rh_idx, norm=True)\n",
    "\n",
    "#--\n",
    "stc_psf_lh_eLORETA = get_point_spread(rm_lor, fwd2['src'], cent_idx_lh_idx, norm=True)\n",
    "\n",
    "stc_ctf_lh_eLORETA = get_cross_talk(rm_lor, fwd2['src'], cent_idx_lh_idx, norm=True)\n",
    "\n",
    "stc_psf_rh_eLORETA = get_point_spread(rm_lor, fwd2['src'], cent_idx_rh_idx, norm=True)\n",
    "\n",
    "stc_ctf_rh_eLORETA = get_cross_talk(rm_lor, fwd2['src'], cent_idx_rh_idx, norm=True)\n",
    "\n",
    "### what if we tried fwd2['src'][0]['rr'][vertno_lh, :] https://mne.tools/stable/auto_tutorials/inverse/10_stc_class.html\n",
    "### vertno_lh = fwd2['src'][0]['vertno'] #vertices in left hemisphere\n",
    "locations_lh = fwd2['src'][0]['rr'][vertno_lh, :]\n",
    "locations_rh = fwd2['src'][1]['rr'][vertno_rh, :]\n",
    "locations = np.vstack([locations_lh, locations_rh])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anat_ple(rm_lor_gen, cent_idx_lh_gen, cent_idx_rh_gen, inverse_gen, vertno_lh, vertno_rh):\n",
    "    # inverse = fwd2['src']\n",
    "    stc_psf_lh_gen = get_point_spread(rm_lor_gen, inverse_gen, cent_idx_lh_gen, norm=True)\n",
    "\n",
    "    stc_ctf_lh_gen = get_cross_talk(rm_lor_gen, inverse_gen, cent_idx_lh_gen, norm=True)\n",
    "\n",
    "    stc_psf_rh_gen = get_point_spread(rm_lor_gen, inverse_gen, cent_idx_rh_gen, norm=True)\n",
    "\n",
    "    stc_ctf_rh_gen = get_cross_talk(rm_lor_gen, inverse_gen, cent_idx_rh_gen, norm=True)\n",
    "\n",
    "    vert_max_ctf_lh, ta_ctf_lh_gen, vert_max_ctf_rh, ta_ctf_rh_gen = stc_argmax(stc_ctf_lh_gen, stc_ctf_rh_gen, vertno_lh, vertno_rh)\n",
    "    vert_max_psf_lh, ta_psf_lh_gen, vert_max_psf_rh, ta_psf_rh_gen = stc_argmax(stc_psf_lh_gen, stc_psf_rh_gen, vertno_lh, vertno_rh)\n",
    "\n",
    "    return vert_max_ctf_lh, ta_ctf_lh_gen, vert_max_ctf_rh, ta_ctf_rh_gen, vert_max_psf_lh, ta_psf_lh_gen, vert_max_psf_rh, ta_psf_rh_gen\n",
    "\n",
    "\n",
    "def stc_argmax(stc_gen_lh, stc_gen_rh, vertno_lh, vertno_rh):\n",
    "    if stc_gen_lh.data.argmax() > 1026:\n",
    "        ta = stc_gen_lh.data\n",
    "        ta_lh_gen = ta[0:1026]\n",
    "        vert_max_lh_gen = vertno_lh[ta_lh_gen.argmax()]\n",
    "    else:\n",
    "        ta_lh_gen = stc_gen_lh.data\n",
    "        vert_max_lh_gen  = vertno_lh[ta_lh_gen.argmax()]\n",
    "   \n",
    "    if stc_gen_rh.data.argmax() > 1026:\n",
    "        ta = stc_gen_rh.data\n",
    "        ta_rh_gen = ta[0:1026]\n",
    "        vert_max_rh_gen = vertno_rh[ta_rh_gen.argmax()]\n",
    "    else:\n",
    "        ta_rh_gen = stc_gen_rh.data\n",
    "        vert_max_rh_gen  = vertno_rh[ta_rh_gen.argmax()]\n",
    "\n",
    "    return vert_max_lh_gen, ta_lh_gen, vert_max_rh_gen, ta_rh_gen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_gen = fwd2['src']\n",
    "vert_max_ctf_lh_mne, ta_ctf_lmne, vert_max_ctf_rh_mne, ta_ctf_rmne, vert_max_psf_lh_mne, ta_psf_lmne, vert_max_psf_rh_mne, ta_psf_rmne = get_anat_ple(rm_mne, cent_idx_lh_idx, cent_idx_rh_idx, inverse_gen, vertno_lh, vertno_rh)\n",
    "vert_max_ctf_lh_dspm, ta_ctf_lds, vert_max_ctf_rh_dspm, ta_ctf_rds, vert_max_psf_lh_dspm, ta_psf_lds, vert_max_psf_rh_dspm, ta_psf_rds = get_anat_ple(rm_dspm, cent_idx_lh_idx, cent_idx_rh_idx, inverse_gen, vertno_lh, vertno_rh)\n",
    "vert_max_ctf_lh_sLORETA, ta_ctf_lsl, vert_max_ctf_rh_sLORETA, ta_ctf_rsl, vert_max_psf_lh_sLORETA, ta_psf_lsl, vert_max_psf_rh_sLORETA, ta_psf_rsl = get_anat_ple(rm_sLORETA, cent_idx_lh_idx, cent_idx_rh_idx, inverse_gen, vertno_lh, vertno_rh)\n",
    "vert_max_ctf_lh_eLORETA, ta_ctf_lel, vert_max_ctf_rh_eLORETA, ta_ctf_rel, vert_max_psf_lh_eLORETA, ta_psf_lel, vert_max_psf_rh_eLORETA, ta_psf_rel = get_anat_ple(rm_lor, cent_idx_lh_idx, cent_idx_rh_idx, inverse_gen, vertno_lh, vertno_rh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vert_max_psf_lh_mne = vertno_lh[stc_psf_lh_mne.data.argmax()]\n",
    "vert_max_ctf_lh_mne = vertno_lh[stc_ctf_lh_mne.data.argmax()]\n",
    "\n",
    "if stc_psf_rh_mne.data.argmax() > 1026:\n",
    "\tta = stc_psf_rh_mne.data\n",
    "\tta_psf_rmne = ta[0:1026]\n",
    "\tvert_max_psf_rh_mne = vertno_rh[ta_psf_rmne.argmax()]\n",
    "else:\n",
    "\tta_psf_rmne = stc_psf_rh_mne.data\n",
    "\tvert_max_psf_rh_mne = vertno_rh[ta_psf_rmne.argmax()]\n",
    "\t\n",
    "if stc_ctf_rh_mne.data.argmax() > 1026:\n",
    "\tta = stc_ctf_rh_mne.data\n",
    "\tta_ctf_rmne = ta[0:1026]\n",
    "\tvert_max_ctf_rh_mne = vertno_rh[ta_psf_rmne.argmax()]\n",
    "else:\n",
    "\tta_ctf_rmne = stc_ctf_rh_mne.data\n",
    "\tvert_max_ctf_rh_mne = vertno_rh[ta_ctf_rmne.argmax()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vert_max_psf_lh_dspm = vertno_lh[stc_psf_lh_dspm.data.argmax()]\n",
    "vert_max_ctf_lh_dspm = vertno_lh[stc_ctf_lh_dspm.data.argmax()]\n",
    "\n",
    "\n",
    "if stc_psf_rh_dspm.data.argmax() > 1026:\n",
    "\tta_psf_rds = stc_psf_rh_dspm.data\n",
    "\tta_psf_rds = ta_psf_rds[0:1026]\n",
    "\tvert_max_psf_rh_dspm = vertno_rh[ta_psf_rds.argmax()]\n",
    "else:\n",
    "\tta_psf_rds  = stc_psf_rh_dspm.data\n",
    "\tvert_max_psf_rh_dspm = vertno_rh[ta_psf_rds.argmax()]\n",
    "\t\n",
    "if stc_ctf_rh_dspm.data.argmax() > 1026:\n",
    "\tta = stc_ctf_rh_dspm.data\n",
    "\tta_ctf_rds = ta[0:1026]\n",
    "\tvert_max_ctf_rh_dspm = vertno_rh[ta_ctf_rds.argmax()]\n",
    "else:\n",
    "\tta_ctf_rds = stc_ctf_rh_dspm.data\n",
    "\tvert_max_ctf_rh_dspm = vertno_rh[ta_ctf_rds.argmax()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vert_max_psf_lh_sLORETA = vertno_lh[stc_psf_lh_sLORETA.data.argmax()]\n",
    "vert_max_ctf_lh_sLORETA = vertno_lh[stc_ctf_lh_sLORETA.data.argmax()]\n",
    "\n",
    "\n",
    "if stc_psf_rh_sLORETA.data.argmax() > 1026:\n",
    "\tta = stc_psf_rh_sLORETA.data\n",
    "\tta_psf_rsl = ta[0:1026]\n",
    "\tvert_max_psf_rh_sLORETA = vertno_rh[ta_psf_rsl.argmax()]\n",
    "else:\n",
    "\tta_psf_rsl = stc_psf_rh_sLORETA.data\n",
    "\tvert_max_psf_rh_sLORETA = vertno_rh[ta_psf_rsl.argmax()]\n",
    "\t\n",
    "if stc_ctf_rh_sLORETA.data.argmax() > 1026:\n",
    "\tta = stc_ctf_rh_sLORETA.data\n",
    "\tta_ctf_rsl = ta[0:1026]\n",
    "\tvert_max_ctf_rh_sLORETA = vertno_rh[ta_ctf_rsl.argmax()]\n",
    "else:\n",
    "\tta_ctf_rsl = stc_ctf_rh_sLORETA.data\n",
    "\tvert_max_ctf_rh_sLORETA = vertno_rh[ta_ctf_rsl.argmax()]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vert_max_psf_lh_eLORETA = vertno_lh[stc_psf_lh_eLORETA.data.argmax()]\n",
    "vert_max_ctf_lh_eLORETA = vertno_lh[stc_ctf_lh_eLORETA.data.argmax()]\n",
    "\n",
    "\n",
    "if stc_psf_rh_eLORETA.data.argmax() > 1026:\n",
    "\tta = stc_psf_rh_eLORETA.data\n",
    "\tta_psf_rel = ta[0:1026]\n",
    "\tvert_max_psf_rh_eLORETA = vertno_rh[ta_psf_rel.argmax()]\n",
    "else:\n",
    "\tta_psf_rel = stc_psf_rh_eLORETA.data\n",
    "\tvert_max_psf_rh_eLORETA = vertno_rh[ta_psf_rel.argmax()]\n",
    "\t\n",
    "if stc_ctf_rh_eLORETA.data.argmax() > 1026:\n",
    "\tta = stc_ctf_rh_eLORETA.data\n",
    "\tta_ctf_rel = ta[0:1026]\n",
    "\tvert_max_ctf_rh_eLORETA = vertno_rh[ta_ctf_rel.argmax()]\n",
    "else:\n",
    "\tta_ctf_rel = stc_ctf_rh_eLORETA.data\n",
    "\tvert_max_ctf_rh_eLORETA = vertno_rh[ta_ctf_rel.argmax()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locerr_total = []\n",
    "#mne\n",
    "true_loc_lh_mne_psf = locations_lh[cent_idx_lh,:]\n",
    "vertmax_loc_lh_mne_psf = locations_lh[stc_psf_lh_mne.data.argmax(),:]\n",
    "diffloc = true_loc_lh_mne_psf - vertmax_loc_lh_mne_psf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'mne_psf_lh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_rh_mne_psf = locations_rh[cent_idx_rh,:]\n",
    "vertmax_loc_rh_mne_psf = locations_rh[ta_psf_rmne.argmax(),:]\n",
    "diffloc = true_loc_rh_mne_psf - vertmax_loc_rh_mne_psf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'mne_psf_rh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_lh_mne_ctf = locations_lh[cent_idx_lh,:]\n",
    "vertmax_loc_lh_mne_ctf = locations_lh[stc_ctf_lh_mne.data.argmax(), :]\n",
    "diffloc = true_loc_lh_mne_ctf - vertmax_loc_lh_mne_ctf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'mne_ctf_lh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_rh_mne_ctf = locations_rh[cent_idx_rh,:]\n",
    "vertmax_loc_rh_mne_ctf = locations_rh[ta_ctf_rmne.argmax(), :]\n",
    "diffloc = true_loc_rh_mne_ctf - vertmax_loc_rh_mne_ctf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'mne_ctf_rh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "#dspm\n",
    "true_loc_lh_dspm_psf = locations_lh[cent_idx_lh,:]\n",
    "vertmax_loc_lh_dspm_psf = locations_lh[stc_psf_lh_dspm.data.argmax(), :]\n",
    "diffloc = true_loc_lh_dspm_psf - vertmax_loc_lh_dspm_psf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'dspm_psf_lh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_rh_dspm_psf = locations_rh[cent_idx_rh,:]\n",
    "vertmax_loc_rh_dspm_psf = locations_rh[ta_psf_rds.argmax(), :]\n",
    "diffloc = true_loc_rh_dspm_psf - vertmax_loc_rh_dspm_psf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'dspm_psf_rh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_lh_dspm_ctf = locations_lh[cent_idx_lh,:]\n",
    "vertmax_loc_lh_dspm_ctf = locations_lh[stc_ctf_lh_dspm.data.argmax(), :]\n",
    "diffloc = true_loc_lh_dspm_ctf - vertmax_loc_lh_dspm_ctf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'dspm_ctf_lh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_rh_dspm_ctf = locations_rh[cent_idx_rh,:]\n",
    "vertmax_loc_rh_dspm_ctf = locations_rh[ta_ctf_rds.argmax(), :]\n",
    "diffloc = true_loc_rh_dspm_ctf - vertmax_loc_rh_dspm_ctf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'dspm_ctf_rh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "\n",
    "#sLORETA\n",
    "\n",
    "true_loc_lh_sLORETA_psf = locations_lh[cent_idx_lh,:]\n",
    "vertmax_loc_lh_sLORETA_psf = locations_lh[stc_psf_lh_sLORETA.data.argmax(), :]\n",
    "diffloc = true_loc_lh_sLORETA_psf - vertmax_loc_lh_sLORETA_psf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'sLORETA_psf_lh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_rh_sLORETA_psf = locations_rh[cent_idx_rh,:]\n",
    "vertmax_loc_rh_sLORETA_psf = locations_rh[ta_psf_rsl.argmax(), :]\n",
    "diffloc = true_loc_rh_sLORETA_psf - vertmax_loc_rh_sLORETA_psf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'sLORETA_psf_rh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_lh_sLORETA_ctf = locations_lh[cent_idx_lh,:]\n",
    "vertmax_loc_lh_sLORETA_ctf = locations_lh[stc_ctf_lh_sLORETA.data.argmax(), :]\n",
    "diffloc = true_loc_lh_sLORETA_ctf - vertmax_loc_lh_sLORETA_ctf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'sLORETA_ctf_lh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_rh_sLORETA_ctf = locations_rh[cent_idx_rh,:]\n",
    "vertmax_loc_rh_sLORETA_ctf = locations_rh[ta_ctf_rsl.argmax(), :]\n",
    "diffloc = true_loc_rh_sLORETA_ctf - vertmax_loc_rh_sLORETA_ctf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'sLORETA_ctf_rh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "\n",
    "#eLORETA\n",
    "true_loc_lh_eLORETA_psf = locations_lh[cent_idx_lh,:]\n",
    "vertmax_loc_lh_eLORETA_psf = locations_lh[stc_psf_lh_eLORETA.data.argmax(), :]\n",
    "diffloc = true_loc_lh_eLORETA_psf - vertmax_loc_lh_eLORETA_psf    # diff btw true locs and maxima locs\n",
    "locerr_e = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'eLORETA_psf_lh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_rh_eLORETA_psf = locations_rh[cent_idx_rh,:]\n",
    "vertmax_loc_rh_eLORETA_psf = locations_rh[ta_psf_rel.argmax(), :]\n",
    "diffloc = true_loc_rh_eLORETA_psf - vertmax_loc_rh_eLORETA_psf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'eLORETA_psf_rh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_lh_eLORETA_ctf = locations_lh[cent_idx_lh,:]\n",
    "vertmax_loc_lh_eLORETA_ctf = locations_lh[stc_ctf_lh_eLORETA.data.argmax(), :]\n",
    "diffloc = true_loc_lh_eLORETA_ctf - vertmax_loc_lh_eLORETA_ctf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'eLORETA_ctf_lh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_rh_eLORETA_ctf = locations_rh[cent_idx_rh,:]\n",
    "vertmax_loc_rh_eLORETA_ctf = locations_rh[ta_ctf_rel.argmax(), :]\n",
    "diffloc = true_loc_rh_eLORETA_ctf - vertmax_loc_rh_eLORETA_ctf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'eLORETA_ctf_rh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "print('######################')\n",
    "for i in range (len(locerr_total)):\n",
    "    \n",
    "    locerr_total[i] = locerr_total[i]*100\n",
    "print(locerr_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locerr_total = []\n",
    "#mne\n",
    "true_loc_lh_mne_psf = locations_lh[cent_idx_lh,:]\n",
    "vertmax_loc_lh_mne_psf = locations_lh[ta_psf_lmne.argmax(),:]\n",
    "diffloc = true_loc_lh_mne_psf - vertmax_loc_lh_mne_psf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'mne_psf_lh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_rh_mne_psf = locations_rh[cent_idx_rh,:]\n",
    "vertmax_loc_rh_mne_psf = locations_rh[ta_psf_rmne.argmax(),:]\n",
    "diffloc = true_loc_rh_mne_psf - vertmax_loc_rh_mne_psf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'mne_psf_rh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_lh_mne_ctf = locations_lh[cent_idx_lh,:]\n",
    "vertmax_loc_lh_mne_ctf = locations_lh[ta_ctf_lmne.argmax(), :]\n",
    "diffloc = true_loc_lh_mne_ctf - vertmax_loc_lh_mne_ctf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'mne_ctf_lh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_rh_mne_ctf = locations_rh[cent_idx_rh,:]\n",
    "vertmax_loc_rh_mne_ctf = locations_rh[ta_ctf_rmne.argmax(), :]\n",
    "diffloc = true_loc_rh_mne_ctf - vertmax_loc_rh_mne_ctf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'mne_ctf_rh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "#dspm\n",
    "true_loc_lh_dspm_psf = locations_lh[cent_idx_lh,:]\n",
    "vertmax_loc_lh_dspm_psf = locations_lh[ta_psf_lds.argmax(), :]\n",
    "diffloc = true_loc_lh_dspm_psf - vertmax_loc_lh_dspm_psf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'dspm_psf_lh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_rh_dspm_psf = locations_rh[cent_idx_rh,:]\n",
    "vertmax_loc_rh_dspm_psf = locations_rh[ta_psf_rds.argmax(), :]\n",
    "diffloc = true_loc_rh_dspm_psf - vertmax_loc_rh_dspm_psf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'dspm_psf_rh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_lh_dspm_ctf = locations_lh[cent_idx_lh,:]\n",
    "vertmax_loc_lh_dspm_ctf = locations_lh[ta_ctf_lds.argmax(), :]\n",
    "diffloc = true_loc_lh_dspm_ctf - vertmax_loc_lh_dspm_ctf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'dspm_ctf_lh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_rh_dspm_ctf = locations_rh[cent_idx_rh,:]\n",
    "vertmax_loc_rh_dspm_ctf = locations_rh[ta_ctf_rds.argmax(), :]\n",
    "diffloc = true_loc_rh_dspm_ctf - vertmax_loc_rh_dspm_ctf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'dspm_ctf_rh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "\n",
    "#sLORETA\n",
    "\n",
    "true_loc_lh_sLORETA_psf = locations_lh[cent_idx_lh,:]\n",
    "vertmax_loc_lh_sLORETA_psf = locations_lh[ta_psf_lsl.argmax(), :]\n",
    "diffloc = true_loc_lh_sLORETA_psf - vertmax_loc_lh_sLORETA_psf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'sLORETA_psf_lh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_rh_sLORETA_psf = locations_rh[cent_idx_rh,:]\n",
    "vertmax_loc_rh_sLORETA_psf = locations_rh[ta_psf_rsl.argmax(), :]\n",
    "diffloc = true_loc_rh_sLORETA_psf - vertmax_loc_rh_sLORETA_psf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'sLORETA_psf_rh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_lh_sLORETA_ctf = locations_lh[cent_idx_lh,:]\n",
    "vertmax_loc_lh_sLORETA_ctf = locations_lh[ta_ctf_lsl.argmax(), :]\n",
    "diffloc = true_loc_lh_sLORETA_ctf - vertmax_loc_lh_sLORETA_ctf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'sLORETA_ctf_lh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_rh_sLORETA_ctf = locations_rh[cent_idx_rh,:]\n",
    "vertmax_loc_rh_sLORETA_ctf = locations_rh[ta_ctf_rsl.argmax(), :]\n",
    "diffloc = true_loc_rh_sLORETA_ctf - vertmax_loc_rh_sLORETA_ctf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'sLORETA_ctf_rh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "\n",
    "#eLORETA\n",
    "true_loc_lh_eLORETA_psf = locations_lh[cent_idx_lh,:]\n",
    "vertmax_loc_lh_eLORETA_psf = locations_lh[ta_psf_lel.argmax(), :]\n",
    "diffloc = true_loc_lh_eLORETA_psf - vertmax_loc_lh_eLORETA_psf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'eLORETA_psf_lh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_rh_eLORETA_psf = locations_rh[cent_idx_rh,:]\n",
    "vertmax_loc_rh_eLORETA_psf = locations_rh[ta_psf_rel.argmax(), :]\n",
    "diffloc = true_loc_rh_eLORETA_psf - vertmax_loc_rh_eLORETA_psf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'eLORETA_psf_rh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_lh_eLORETA_ctf = locations_lh[cent_idx_lh,:]\n",
    "vertmax_loc_lh_eLORETA_ctf = locations_lh[ta_ctf_lel.argmax(), :]\n",
    "diffloc = true_loc_lh_eLORETA_ctf - vertmax_loc_lh_eLORETA_ctf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'eLORETA_ctf_lh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "true_loc_rh_eLORETA_ctf = locations_rh[cent_idx_rh,:]\n",
    "vertmax_loc_rh_eLORETA_ctf = locations_rh[ta_ctf_rel.argmax(), :]\n",
    "diffloc = true_loc_rh_eLORETA_ctf - vertmax_loc_rh_eLORETA_ctf    # diff btw true locs and maxima locs\n",
    "locerr = np.linalg.norm(diffloc, axis=0)  # Euclidean distance\n",
    "print(locerr, 'eLORETA_ctf_rh')\n",
    "locerr_total.append(locerr)\n",
    "\n",
    "print('######################')\n",
    "for i in range (len(locerr_total)):\n",
    "    \n",
    "    locerr_total[i] = locerr_total[i]*100\n",
    "print(locerr_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_ar = epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin = 0.0\n",
    "tmax = 0.4\n",
    "fmin = 1.\n",
    "fmax = 90.\n",
    "sfreq = epochs_ar.info['sfreq']\n",
    "\n",
    "spectrum = epochs_ar.compute_psd(\n",
    "    'welch',\n",
    "    n_fft=int(sfreq * (tmax - tmin)),\n",
    "    n_overlap=0, n_per_seg=None,\n",
    "    tmin=tmin, tmax=tmax,\n",
    "    fmin=fmin, fmax=fmax,\n",
    "    window='boxcar',\n",
    "    verbose=False)\n",
    "psds, freqs = spectrum.get_data(return_freqs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snr_spectrum(psd, noise_n_neighbor_freqs=1, noise_skip_neighbor_freqs=1):\n",
    "    \"\"\"Compute SNR spectrum from PSD spectrum using convolution.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    psd : ndarray, shape ([n_trials, n_channels,] n_frequency_bins)\n",
    "        Data object containing PSD values. Works with arrays as produced by\n",
    "        MNE's PSD functions or channel/trial subsets.\n",
    "    noise_n_neighbor_freqs : int\n",
    "        Number of neighboring frequencies used to compute noise level.\n",
    "        increment by one to add one frequency bin ON BOTH SIDES\n",
    "    noise_skip_neighbor_freqs : int\n",
    "        set this >=1 if you want to exclude the immediately neighboring\n",
    "        frequency bins in noise level calculation\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    snr : ndarray, shape ([n_trials, n_channels,] n_frequency_bins)\n",
    "        Array containing SNR for all epochs, channels, frequency bins.\n",
    "        NaN for frequencies on the edges, that do not have enough neighbors on\n",
    "        one side to calculate SNR.\n",
    "    \"\"\"\n",
    "    # Construct a kernel that calculates the mean of the neighboring\n",
    "    # frequencies\n",
    "    averaging_kernel = np.concatenate((\n",
    "        np.ones(noise_n_neighbor_freqs),\n",
    "        np.zeros(2 * noise_skip_neighbor_freqs + 1),\n",
    "        np.ones(noise_n_neighbor_freqs)))\n",
    "    averaging_kernel /= averaging_kernel.sum()\n",
    "\n",
    "    # Calculate the mean of the neighboring frequencies by convolving with the\n",
    "    # averaging kernel.\n",
    "    mean_noise = np.apply_along_axis(\n",
    "        lambda psd_: np.convolve(psd_, averaging_kernel, mode='valid'),\n",
    "        axis=-1, arr=psd\n",
    "    )\n",
    "\n",
    "    # The mean is not defined on the edges so we will pad it with nan's. The\n",
    "    # padding needs to be done for the last dimension only so we set it to\n",
    "    # (0, 0) for the other ones.\n",
    "    edge_width = noise_n_neighbor_freqs + noise_skip_neighbor_freqs\n",
    "    pad_width = [(0, 0)] * (mean_noise.ndim - 1) + [(edge_width, edge_width)]\n",
    "    mean_noise = np.pad(\n",
    "        mean_noise, pad_width=pad_width, constant_values=np.nan\n",
    "    )\n",
    "\n",
    "    return psd / mean_noise, mean_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snrs, mean_noise = snr_spectrum(psds, noise_n_neighbor_freqs=1,\n",
    "                    noise_skip_neighbor_freqs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, sharex='all', sharey='none', figsize=(8, 5))\n",
    "freq_range = range(np.where(np.floor(freqs) == 2.)[0][0],\n",
    "                   np.where(np.ceil(freqs) == fmax - 2)[0][0])\n",
    "# freq_range = range(1,30)\n",
    "\n",
    "#freq_range = range(0,24)\n",
    "psds_plot = 10 * np.log10(psds)\n",
    "psds_mean = psds_plot.mean(axis=(0, 1))[freq_range]\n",
    "psds_std = psds_plot.std(axis=(0, 1))[freq_range]\n",
    "axes[0].plot(freqs[freq_range], psds_mean, color='b')\n",
    "axes[0].fill_between(\n",
    "    freqs[freq_range], psds_mean - psds_std, psds_mean + psds_std,\n",
    "    color='b', alpha=.2)\n",
    "axes[0].set(title=\"PSD spectrum\", ylabel='Power Spectral Density [dB]')\n",
    "\n",
    "# SNR spectrum\n",
    "\n",
    "snr_mean = snrs.mean(axis=(0, 1))[freq_range]\n",
    "snr_std = snrs.std(axis=(0, 1))[freq_range]\n",
    "\n",
    "axes[1].plot(freqs[freq_range], snr_mean, color='r')\n",
    "axes[1].fill_between(\n",
    "    freqs[freq_range], snr_mean - snr_std, snr_mean + snr_std,\n",
    "    color='r', alpha=.2)\n",
    "axes[1].set(\n",
    "    title=\"SNR spectrum\", xlabel='Frequency [Hz]',\n",
    "    ylabel='SNR', ylim=[-2, 20], xlim=[fmin, fmax])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(freqs)\n",
    "freq_range = range(np.where(np.floor(freqs) == 2.)[0][0],\n",
    "                 np.where(np.ceil(freqs) == fmax - 2)[0][0])\n",
    "freq_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snr_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_snr = np.mean(snr_mean[2:])\n",
    "average_snr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('mne')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e1fcaa064cd0107f7878d4f34cdffb5ddb7718cbbefb88f7c2981c8f5e5cbb43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
